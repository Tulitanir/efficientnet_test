{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tulitanir/efficientnet_test/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pylab as plt\n",
    "from math import ceil\n",
    "from tqdm import tqdm\n",
    "import gradio as gr\n",
    "import os\n",
    "import gc\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'train': 'dataset/labels/train.csv',\n",
    "    'test': 'dataset/labels/val.csv',\n",
    "    'classnames': 'dataset/labels/classid_classname.csv'\n",
    "}\n",
    "\n",
    "efficient_net_config = {\n",
    "    \"b0\" : (1.0, 1.0, 224, 0.2),\n",
    "    \"b1\" : (1.0, 1.1, 240, 0.2),\n",
    "    # \"b2\" : (1.1, 1.2, 260, 0.3),\n",
    "    # \"b3\" : (1.2, 1.4, 300, 0.3),\n",
    "    # \"b4\" : (1.4, 1.8, 380, 0.4),\n",
    "    # \"b5\" : (1.6, 2.2, 456, 0.4),\n",
    "    # \"b6\" : (1.8, 2.6, 528, 0.5),\n",
    "    # \"b7\" : (2.0, 3.1, 600, 0.5)\n",
    "}\n",
    "\n",
    "image_size = 224\n",
    "num_classes = 3263\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = None\n",
    "batch_size = 32\n",
    "\n",
    "def read_image(path):\n",
    "    return torchvision.io.read_image(f'dataset/fullMin256/{path}') / 255.0\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, config, train=True):\n",
    "        self.dataframe = pd.read_csv(config['train']) if train else pd.read_csv(config['test'])\n",
    "        self.transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "    \n",
    "        image = read_image(self.dataframe.iloc[index]['path'])\n",
    "        image = self.transforms(image)\n",
    "        class_id = torch.tensor(self.dataframe.iloc[index]['class_id'])\n",
    "        return image, class_id\n",
    "\n",
    "# train_dataset = CustomDataset(config)\n",
    "\n",
    "# test_dataset = CustomDataset(config, False)\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def check_loader(loader):\n",
    "    dataframe = pd.read_csv(config['classnames'])\n",
    "    \n",
    "    (img, label) = next(iter(loader))\n",
    "    _, ax = plt.subplots(2, 2, figsize=(8, 8))\n",
    "    plt.suptitle('Images')\n",
    "    index = 0\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            csv_index = label[index].item()\n",
    "            title = dataframe.iloc[csv_index]['class_name']\n",
    "            ax[i][j].imshow((img[index].permute(1, 2, 0)))\n",
    "            ax[i][j].set_title(title, fontfamily='serif', fontsize='medium')\n",
    "            index+= 1\n",
    "            \n",
    "# check_loader(train_dataloader)\n",
    "# check_loader(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, n_in, n_out, kernel_size = 3, stride = 1, \n",
    "                 padding = 0, groups = 1, bn = True, act = True,\n",
    "                 bias = False\n",
    "                ):\n",
    "        \n",
    "        super(CNNBlock, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(n_in, n_out, kernel_size = kernel_size,\n",
    "                              stride = stride, padding = padding,\n",
    "                              groups = groups, bias = bias\n",
    "                             )\n",
    "        self.batch_norm = nn.BatchNorm2d(n_out) if bn else nn.Identity()\n",
    "        self.activation = nn.SiLU() if act else nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class SqueezeExcitation(nn.Module):\n",
    "    def __init__(self, n_in, reduced_dim):\n",
    "        super(SqueezeExcitation, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.seq = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(n_in, reduced_dim, kernel_size=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(reduced_dim, n_in, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "       \n",
    "    def forward(self, x):\n",
    "        y = self.seq(x)\n",
    "        return x * y\n",
    "    \n",
    "class StochasticDepth(nn.Module):\n",
    "    def __init__(self, survival_prob = 0.8):\n",
    "        super(StochasticDepth, self).__init__()\n",
    "        \n",
    "        self.p =  survival_prob\n",
    "        \n",
    "    def forward(self, x):   \n",
    "        if not self.training:\n",
    "            return x\n",
    "        \n",
    "        binary_tensor = torch.rand(x.shape[0], 1, 1, 1, device=x.device) < self.p\n",
    "        \n",
    "        return torch.div(x, self.p) * binary_tensor\n",
    "    \n",
    "class MBConvN(nn.Module):\n",
    "    def __init__(self, n_in, n_out, kernel_size = 3, \n",
    "                 stride = 1, expansion_factor = 6,\n",
    "                 reduction = 4, # Squeeze and Excitation Block\n",
    "                 survival_prob = 0.8 # Stochastic Depth\n",
    "                ):\n",
    "        \n",
    "        super(MBConvN, self).__init__()\n",
    "        \n",
    "        self.skip_connection = (stride == 1 and n_in == n_out) \n",
    "        intermediate_channels = int(n_in * expansion_factor)\n",
    "        padding = (kernel_size - 1)//2\n",
    "        reduced_dim = int(n_in//reduction)\n",
    "        \n",
    "        self.expand = nn.Identity() if (expansion_factor == 1) else CNNBlock(n_in, intermediate_channels, kernel_size = 1)\n",
    "        self.depthwise_conv = CNNBlock(intermediate_channels, intermediate_channels,\n",
    "                                        kernel_size = kernel_size, stride = stride, \n",
    "                                        padding = padding, groups = intermediate_channels\n",
    "                                       )\n",
    "        self.se = SqueezeExcitation(intermediate_channels, reduced_dim = reduced_dim)\n",
    "        self.pointwise_conv = CNNBlock(intermediate_channels, n_out, \n",
    "                                        kernel_size = 1, act = False\n",
    "                                       )\n",
    "        self.drop_layers = StochasticDepth(survival_prob = survival_prob)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        x = self.expand(x)\n",
    "        x = self.depthwise_conv(x)\n",
    "        x = self.se(x)\n",
    "        x = self.pointwise_conv(x)\n",
    "        \n",
    "        if self.skip_connection:\n",
    "            x = self.drop_layers(x)\n",
    "            x += residual\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self, width_mult = 1, depth_mult = 1, \n",
    "                 dropout_rate = 0.2, num_classes = num_classes):\n",
    "        super(EfficientNet, self).__init__()\n",
    "        \n",
    "        last_channel = ceil(1280 * width_mult)\n",
    "        self.features = self._feature_extractor(width_mult, depth_mult, last_channel)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(last_channel, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.classifier(x.view(x.shape[0], -1))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "        \n",
    "    def _feature_extractor(self, width_mult, depth_mult, last_channel):\n",
    "        channels = 4*ceil(int(32*width_mult) / 4)\n",
    "        layers = [CNNBlock(3, channels, kernel_size = 3, stride = 2, padding = 1)]\n",
    "        in_channels = channels\n",
    "        \n",
    "        kernels = [3, 3, 5, 3, 5, 5, 3]\n",
    "        expansions = [1, 6, 6, 6, 6, 6, 6]\n",
    "        num_channels = [16, 24, 40, 80, 112, 192, 320]\n",
    "        num_layers = [1, 2, 2, 3, 3, 4, 1]\n",
    "        strides =[1, 2, 2, 2, 1, 2, 1]\n",
    "        \n",
    "        scaled_num_channels = [4*ceil(int(c*width_mult) / 4) for c in num_channels]\n",
    "        scaled_num_layers = [int(d * depth_mult) for d in num_layers]\n",
    "\n",
    "        \n",
    "        for i in range(len(scaled_num_channels)):\n",
    "             \n",
    "            layers += [MBConvN(in_channels if repeat==0 else scaled_num_channels[i], \n",
    "                               scaled_num_channels[i],\n",
    "                               kernel_size = kernels[i],\n",
    "                               stride = strides[i] if repeat==0 else 1, \n",
    "                               expansion_factor = expansions[i]\n",
    "                              )\n",
    "                       for repeat in range(scaled_num_layers[i])\n",
    "                      ]\n",
    "            in_channels = scaled_num_channels[i]\n",
    "        \n",
    "        layers.append(CNNBlock(in_channels, last_channel, kernel_size = 1, stride = 1, padding = 0))\n",
    "    \n",
    "        return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(num_of_epochs, track_training, track_test, save_path, loss=True):\n",
    "    label_end = 'Loss' if loss else 'Accuracy'\n",
    "    plt.plot(range(1, num_of_epochs+2), track_training, label=f'Training {label_end}')\n",
    "    plt.plot(range(1, num_of_epochs+2), track_test, label=f'Testing {label_end}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(f'{label_end}')\n",
    "    plt.title(f'Training and testing {label_end}')\n",
    "    plt.legend()\n",
    "    \n",
    "    path = os.path.join(save_path, 'plots')\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    plt.savefig(os.path.join(path, f'{num_of_epochs}_{label_end}_plots.png'))\n",
    "    plt.close()\n",
    "    \n",
    "def save_model(model, epoch, accuracy, loss, save_path):\n",
    "    additional_info = {\n",
    "        'epoch': epoch,\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "    \n",
    "    path = os.path.join(save_path, 'models')\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    path = os.path.join(path, f'efficient_net_{epoch}.pth')\n",
    "        \n",
    "    torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'additional_info': additional_info\n",
    "        },      \n",
    "        path\n",
    "    )\n",
    "    \n",
    "def unload_model(model):\n",
    "    model.cpu()\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return 'Модель выгружена из памяти'\n",
    "    \n",
    "def load_model(path):\n",
    "    checkpoint = torch.load(path)\n",
    "    version = 'b0'\n",
    "    width_mult, depth_mult, res, dropout_rate = efficient_net_config[version]\n",
    "    model = EfficientNet(width_mult, depth_mult, dropout_rate, num_classes = num_classes)\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def inference(image):\n",
    "    path = '/run/media/tulitanir/Новый том/efficientnet_output/models/efficient_net_31.pth'\n",
    "    transformations = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "    image = transformations(image).to(device)\n",
    "    image = image[None, :, :, :]\n",
    "    model = load_model(path)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "    unload_model(model)\n",
    "    probs = F.softmax(outputs[0], dim=0)\n",
    "    \n",
    "    dataframe = pd.read_csv(config['classnames'])\n",
    "        \n",
    "    preds, indexes = torch.topk(probs, k=5)\n",
    "    return {dataframe.iloc[i.item()]['class_name']: p.item() for i, p in zip(indexes, preds)}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'File' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/efficientnet_test/venv/lib/python3.10/site-packages/torch/serialization.py:540\u001b[0m, in \u001b[0;36m_check_seekable\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 540\u001b[0m     \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseek\u001b[49m(f\u001b[38;5;241m.\u001b[39mtell())\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'File' object has no attribute 'seek'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m gr\u001b[38;5;241m.\u001b[39mBlocks() \u001b[38;5;28;01mas\u001b[39;00m demo:\n\u001b[0;32m----> 2\u001b[0m     pth \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mЗагрузить модель\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msingle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfilepath\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     image \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mImage(width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1280\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m720\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mЗагрузить изображение\u001b[39m\u001b[38;5;124m'\u001b[39m, min_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m224\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpil\u001b[39m\u001b[38;5;124m'\u001b[39m, image_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m     output \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mLabel(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mВывод\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 45\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(path):\n\u001b[0;32m---> 45\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     version \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     47\u001b[0m     width_mult, depth_mult, res, dropout_rate \u001b[38;5;241m=\u001b[39m efficient_net_config[version]\n",
      "File \u001b[0;32m~/efficientnet_test/venv/lib/python3.10/site-packages/torch/serialization.py:997\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    995\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 997\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    999\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/efficientnet_test/venv/lib/python3.10/site-packages/torch/serialization.py:449\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _open_buffer_writer(name_or_buffer)\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_buffer_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in mode but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/efficientnet_test/venv/lib/python3.10/site-packages/torch/serialization.py:434\u001b[0m, in \u001b[0;36m_open_buffer_reader.__init__\u001b[0;34m(self, buffer)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, buffer):\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(buffer)\n\u001b[0;32m--> 434\u001b[0m     \u001b[43m_check_seekable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/efficientnet_test/venv/lib/python3.10/site-packages/torch/serialization.py:543\u001b[0m, in \u001b[0;36m_check_seekable\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (io\u001b[38;5;241m.\u001b[39mUnsupportedOperation, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 543\u001b[0m     \u001b[43mraise_err_msg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseek\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/efficientnet_test/venv/lib/python3.10/site-packages/torch/serialization.py:536\u001b[0m, in \u001b[0;36m_check_seekable.<locals>.raise_err_msg\u001b[0;34m(patterns, e)\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n\u001b[1;32m    533\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. You can only torch.load from a file that is seekable.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    534\u001b[0m                         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please pre-load the data into a buffer like io.BytesIO and\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    535\u001b[0m                         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m try to load from it instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 536\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(msg)\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'File' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead."
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    image = gr.Image(width=1280, height=720, label='Загрузить изображение', min_width=224, type='pil', image_mode='RGB')\n",
    "    output = gr.Label(label=\"Вывод\")\n",
    "    greet_btn = gr.Button(\"Inference\")\n",
    "    greet_btn.click(fn=inference, inputs=[image], outputs=[output])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_and_accuracy(model, dataloader, size_of_dataset, criterion):\n",
    "    running_loss = 0\n",
    "    running_accuracy = 0\n",
    "    \n",
    "    for (inputs, labels) in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            _ , preds = torch.max(outputs, 1)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_accuracy += torch.sum(preds == labels.data)\n",
    "        \n",
    "    epoch_loss = running_loss / size_of_dataset\n",
    "    epoch_accuracy = running_accuracy / size_of_dataset\n",
    "    \n",
    "    return epoch_loss, epoch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, num_of_epochs, root_path):\n",
    "    track_training_loss = []\n",
    "    track_test_loss = []\n",
    "    track_training_accuracy = []\n",
    "    track_test_accuracy = []\n",
    "\n",
    "    for epoch in range(num_of_epochs):\n",
    "        loop = tqdm(train_dataloader, leave=True)\n",
    "        print(f'\\nEpoch {epoch + 1}/{num_of_epochs}')\n",
    "        print('-'*30)\n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        running_accuracy = 0\n",
    "\n",
    "        for _, (inputs, labels) in enumerate(loop):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _ , preds = torch.max(outputs, 1)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()*inputs.size(0)\n",
    "            running_accuracy += torch.sum(preds == labels.data)\n",
    "            \n",
    "            loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_dataset)\n",
    "        epoch_accuracy = running_accuracy / len(train_dataset)\n",
    "        \n",
    "        track_training_accuracy.append(epoch_accuracy.item())\n",
    "        track_training_loss.append(epoch_loss)\n",
    "\n",
    "        print(f'Training Loss: {epoch_loss:.4f} Training Acc.: {epoch_accuracy:.4f}')\n",
    "        model.eval()\n",
    "\n",
    "        test_loss, test_accuracy = calculate_loss_and_accuracy(model, test_dataloader, len(test_dataset), criterion)\n",
    "        track_test_loss.append(test_loss)\n",
    "        track_test_accuracy.append(test_accuracy.item())\n",
    "        \n",
    "        plot(epoch, track_training=track_training_loss, track_test=track_test_loss, save_path=root_path)\n",
    "        plot(epoch, track_training=track_training_accuracy, track_test=track_test_accuracy, save_path=root_path, loss=False)\n",
    "        save_model(model=model, epoch=epoch, accuracy=test_accuracy, loss=test_loss, save_path=root_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version = 'b0'\n",
    "# width_mult, depth_mult, res, dropout_rate = efficient_net_config[version]\n",
    "# model = EfficientNet(width_mult, depth_mult, dropout_rate, num_classes = num_classes)\n",
    "# model = model.to(device)\n",
    "\n",
    "# epoch_num = 100\n",
    "# lr = 3e-4\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# train(model = model,\n",
    "#       criterion = criterion,\n",
    "#         optimizer = optimizer,\n",
    "#         num_of_epochs = epoch_num,\n",
    "#         root_path='/run/media/tulitanir/Новый том/efficientnet_output/')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
