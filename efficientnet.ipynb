{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pylab as plt\n",
    "from math import ceil\n",
    "from tqdm import tqdm\n",
    "import gradio as gr\n",
    "import os\n",
    "import gc\n",
    "from PIL import Image\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'train': 'dataset/labels/train.csv',\n",
    "    'test': 'dataset/labels/val.csv',\n",
    "    'classnames': 'dataset/labels/classid_classname.csv'\n",
    "}\n",
    "\n",
    "efficient_net_config = {\n",
    "    \"b0\" : (1.0, 1.0, 224, 0.2),\n",
    "    \"b1\" : (1.0, 1.1, 240, 0.2),\n",
    "    # \"b2\" : (1.1, 1.2, 260, 0.3),\n",
    "    # \"b3\" : (1.2, 1.4, 300, 0.3),\n",
    "    # \"b4\" : (1.4, 1.8, 380, 0.4),\n",
    "    # \"b5\" : (1.6, 2.2, 456, 0.4),\n",
    "    # \"b6\" : (1.8, 2.6, 528, 0.5),\n",
    "    # \"b7\" : (2.0, 3.1, 600, 0.5)\n",
    "}\n",
    "\n",
    "image_size = 240\n",
    "num_classes = 3263\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = None\n",
    "batch_size = 64\n",
    "\n",
    "def read_image(path):\n",
    "    return torchvision.io.read_image(f'dataset/fullMin256/{path}') / 255.0\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, config, train=True):\n",
    "        self.dataframe = pd.read_csv(config['train']) if train else pd.read_csv(config['test'])\n",
    "        self.transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]\n",
    "    )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "    \n",
    "        image = read_image(self.dataframe.iloc[index]['path'])\n",
    "        image = self.transforms(image)\n",
    "        class_id = torch.tensor(self.dataframe.iloc[index]['class_id'])\n",
    "        return image, class_id\n",
    "\n",
    "train_dataset = CustomDataset(config)\n",
    "\n",
    "test_dataset = CustomDataset(config, False)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def check_loader(loader):\n",
    "    dataframe = pd.read_csv(config['classnames'])\n",
    "    \n",
    "    (img, label) = next(iter(loader))\n",
    "    _, ax = plt.subplots(2, 2, figsize=(8, 8))\n",
    "    plt.suptitle('Images')\n",
    "    index = 0\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            csv_index = label[index].item()\n",
    "            title = dataframe.iloc[csv_index]['class_name']\n",
    "            ax[i][j].imshow((img[index].permute(1, 2, 0)))\n",
    "            ax[i][j].set_title(title, fontfamily='serif', fontsize='medium')\n",
    "            index+= 1\n",
    "            \n",
    "# check_loader(train_dataloader)\n",
    "# check_loader(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, n_in, n_out, kernel_size = 3, stride = 1, \n",
    "                 padding = 0, groups = 1, bn = True, act = True,\n",
    "                 bias = False\n",
    "                ):\n",
    "        \n",
    "        super(CNNBlock, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(n_in, n_out, kernel_size = kernel_size,\n",
    "                              stride = stride, padding = padding,\n",
    "                              groups = groups, bias = bias\n",
    "                             )\n",
    "        self.batch_norm = nn.BatchNorm2d(n_out) if bn else nn.Identity()\n",
    "        self.activation = nn.SiLU() if act else nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class SqueezeExcitation(nn.Module):\n",
    "    def __init__(self, n_in, reduced_dim):\n",
    "        super(SqueezeExcitation, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.seq = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(n_in, reduced_dim, kernel_size=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(reduced_dim, n_in, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "       \n",
    "    def forward(self, x):\n",
    "        y = self.seq(x)\n",
    "        return x * y\n",
    "    \n",
    "class StochasticDepth(nn.Module):\n",
    "    def __init__(self, survival_prob = 0.8):\n",
    "        super(StochasticDepth, self).__init__()\n",
    "        \n",
    "        self.p =  survival_prob\n",
    "        \n",
    "    def forward(self, x):   \n",
    "        if not self.training:\n",
    "            return x\n",
    "        \n",
    "        binary_tensor = torch.rand(x.shape[0], 1, 1, 1, device=x.device) < self.p\n",
    "        \n",
    "        return torch.div(x, self.p) * binary_tensor\n",
    "    \n",
    "class MBConvN(nn.Module):\n",
    "    def __init__(self, n_in, n_out, kernel_size = 3, \n",
    "                 stride = 1, expansion_factor = 6,\n",
    "                 reduction = 4, # Squeeze and Excitation Block\n",
    "                 survival_prob = 0.8 # Stochastic Depth\n",
    "                ):\n",
    "        \n",
    "        super(MBConvN, self).__init__()\n",
    "        \n",
    "        self.skip_connection = (stride == 1 and n_in == n_out) \n",
    "        intermediate_channels = int(n_in * expansion_factor)\n",
    "        padding = (kernel_size - 1)//2\n",
    "        reduced_dim = int(n_in//reduction)\n",
    "        \n",
    "        self.expand = nn.Identity() if (expansion_factor == 1) else CNNBlock(n_in, intermediate_channels, kernel_size = 1)\n",
    "        self.depthwise_conv = CNNBlock(intermediate_channels, intermediate_channels,\n",
    "                                        kernel_size = kernel_size, stride = stride, \n",
    "                                        padding = padding, groups = intermediate_channels\n",
    "                                       )\n",
    "        self.se = SqueezeExcitation(intermediate_channels, reduced_dim = reduced_dim)\n",
    "        self.pointwise_conv = CNNBlock(intermediate_channels, n_out, \n",
    "                                        kernel_size = 1, act = False\n",
    "                                       )\n",
    "        self.drop_layers = StochasticDepth(survival_prob = survival_prob)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        x = self.expand(x)\n",
    "        x = self.depthwise_conv(x)\n",
    "        x = self.se(x)\n",
    "        x = self.pointwise_conv(x)\n",
    "        \n",
    "        if self.skip_connection:\n",
    "            x = self.drop_layers(x)\n",
    "            x += residual\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self, width_mult = 1, depth_mult = 1, \n",
    "                 dropout_rate = 0.2, num_classes = num_classes):\n",
    "        super(EfficientNet, self).__init__()\n",
    "        \n",
    "        last_channel = ceil(1280 * width_mult)\n",
    "        self.features = self._feature_extractor(width_mult, depth_mult, last_channel)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(last_channel, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.classifier(x.view(x.shape[0], -1))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "        \n",
    "    def _feature_extractor(self, width_mult, depth_mult, last_channel):\n",
    "        channels = 4*ceil(int(32*width_mult) / 4)\n",
    "        layers = [CNNBlock(3, channels, kernel_size = 3, stride = 2, padding = 1)]\n",
    "        in_channels = channels\n",
    "        \n",
    "        kernels = [3, 3, 5, 3, 5, 5, 3]\n",
    "        expansions = [1, 6, 6, 6, 6, 6, 6]\n",
    "        num_channels = [16, 24, 40, 80, 112, 192, 320]\n",
    "        num_layers = [1, 2, 2, 3, 3, 4, 1]\n",
    "        strides =[1, 2, 2, 2, 1, 2, 1]\n",
    "        \n",
    "        scaled_num_channels = [4*ceil(int(c*width_mult) / 4) for c in num_channels]\n",
    "        scaled_num_layers = [int(d * depth_mult) for d in num_layers]\n",
    "\n",
    "        \n",
    "        for i in range(len(scaled_num_channels)):\n",
    "             \n",
    "            layers += [MBConvN(in_channels if repeat==0 else scaled_num_channels[i], \n",
    "                               scaled_num_channels[i],\n",
    "                               kernel_size = kernels[i],\n",
    "                               stride = strides[i] if repeat==0 else 1, \n",
    "                               expansion_factor = expansions[i]\n",
    "                              )\n",
    "                       for repeat in range(scaled_num_layers[i])\n",
    "                      ]\n",
    "            in_channels = scaled_num_channels[i]\n",
    "        \n",
    "        layers.append(CNNBlock(in_channels, last_channel, kernel_size = 1, stride = 1, padding = 0))\n",
    "    \n",
    "        return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def plot(num_of_epochs, track_training, track_test, save_path, loss=True):\n",
    "    label_end = 'Loss' if loss else 'Accuracy'\n",
    "    plt.plot(range(1, num_of_epochs+2), track_training, label=f'Training {label_end}')\n",
    "    plt.plot(range(1, num_of_epochs+2), track_test, label=f'Testing {label_end}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(f'{label_end}')\n",
    "    plt.title(f'Training and testing {label_end}')\n",
    "    plt.legend()\n",
    "    \n",
    "    path = os.path.join(save_path, 'plots')\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    plt.savefig(os.path.join(path, f'{num_of_epochs}_{label_end}_plots.png'))\n",
    "    plt.close()\n",
    "    \n",
    "def save_model(model, epoch, accuracy, loss, save_path):\n",
    "    additional_info = {\n",
    "        'epoch': epoch,\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "    \n",
    "    path = os.path.join(save_path, 'models')\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    path = os.path.join(path, f'efficient_net_{epoch}.pth')\n",
    "        \n",
    "    torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'additional_info': additional_info\n",
    "        },      \n",
    "        path\n",
    "    )\n",
    "    \n",
    "def get_checkpoints(directory):\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.pth')]\n",
    "\n",
    "checkpoint_directory = \"checkpoints\"\n",
    "\n",
    "checkpoints = get_checkpoints(checkpoint_directory)\n",
    "    \n",
    "def unload_model(model):\n",
    "    model.cpu()\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return 'Модель выгружена из памяти'\n",
    "    \n",
    "def load_model(path):\n",
    "    checkpoint = torch.load(path)\n",
    "    version = 'b1'\n",
    "    width_mult, depth_mult, res, dropout_rate = efficient_net_config[version]\n",
    "    model = EfficientNet(width_mult, depth_mult, dropout_rate, num_classes = num_classes)\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def inference(image, model, image_size):\n",
    "    transformations = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]\n",
    "    )\n",
    "    image = transformations(image).to(device)\n",
    "    image = image[None, :, :, :]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "    probs = F.softmax(outputs[0], dim=0)\n",
    "    \n",
    "    dataframe = pd.read_csv(config['classnames'])\n",
    "        \n",
    "    preds, indexes = torch.topk(probs, k=5)\n",
    "    return {dataframe.iloc[i.item()]['class_name']: p.item() for i, p in zip(indexes, preds)}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "current_model = None\n",
    "\n",
    "def set_model_and_resolution(checkpoint, resolution):\n",
    "    global current_model\n",
    "    if current_model is not None:\n",
    "        unload_model(current_model)\n",
    "    current_model = load_model(checkpoint)\n",
    "    return f\"Загружена модель из чекпоинта {checkpoint}\"\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    \n",
    "    with gr.Row():\n",
    "        checkpoint_dropdown = gr.Dropdown(choices=checkpoints, label=\"Выберите чекпоинт\")\n",
    "        # resolution_dropdown = gr.Dropdown(choices=[224, 240], label=\"Выберите разрешение изображения\")\n",
    "        load_button = gr.Button(\"Загрузка модели\")\n",
    "        load_output = gr.Textbox(label=\"Статус\", interactive=False)\n",
    "    \n",
    "    load_button.click(fn=set_model_and_resolution, inputs=[checkpoint_dropdown], outputs=[load_output])\n",
    "    \n",
    "    with gr.Row():\n",
    "        image_input = gr.Image(type=\"pil\", image_mode=\"RGB\", label=\"Загрузите изображение\", sources=['clipboard'])\n",
    "        output = gr.Label(label=\"Предсказания\")\n",
    "        run_button = gr.Button(\"Run Inference\")\n",
    "    \n",
    "    def run_inference(image_info):\n",
    "        return inference(image_info, current_model, 240)\n",
    "    \n",
    "    run_button.click(fn=run_inference, inputs=[image_input], outputs=output)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def calculate_loss_and_accuracy(model, dataloader, size_of_dataset, criterion):\n",
    "    running_loss = 0\n",
    "    running_accuracy = 0\n",
    "    \n",
    "    for (inputs, labels) in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            _ , preds = torch.max(outputs, 1)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_accuracy += torch.sum(preds == labels.data)\n",
    "        \n",
    "    epoch_loss = running_loss / size_of_dataset\n",
    "    epoch_accuracy = running_accuracy / size_of_dataset\n",
    "    print(f'Test Loss: {epoch_loss:.4f} Test Acc.: {epoch_accuracy:.4f}')\n",
    "    return epoch_loss, epoch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, scheduler, num_of_epochs, root_path):\n",
    "    track_training_loss = []\n",
    "    track_test_loss = []\n",
    "    track_training_accuracy = []\n",
    "    track_test_accuracy = []\n",
    "\n",
    "    for epoch in range(num_of_epochs):\n",
    "        loop = tqdm(train_dataloader, leave=True)\n",
    "        print(f'\\nEpoch {epoch + 1}/{num_of_epochs}')\n",
    "        print('-'*30)\n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        running_accuracy = 0\n",
    "\n",
    "        for _, (inputs, labels) in enumerate(loop):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with autocast(dtype=torch.bfloat16):\n",
    "                outputs = model(inputs)\n",
    "                _ , preds = torch.max(outputs, 1)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item()*inputs.size(0)\n",
    "            running_accuracy += torch.sum(preds == labels.data)\n",
    "            \n",
    "            loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_dataset)\n",
    "        epoch_accuracy = running_accuracy / len(train_dataset)\n",
    "        \n",
    "        track_training_accuracy.append(epoch_accuracy.item())\n",
    "        track_training_loss.append(epoch_loss)\n",
    "\n",
    "        print(f'Training Loss: {epoch_loss:.4f} Training Acc.: {epoch_accuracy:.4f}')\n",
    "        model.eval()\n",
    "\n",
    "        test_loss, test_accuracy = calculate_loss_and_accuracy(model, test_dataloader, len(test_dataset), criterion)\n",
    "        track_test_loss.append(test_loss)\n",
    "        track_test_accuracy.append(test_accuracy.item())\n",
    "        \n",
    "        plot(epoch, track_training=track_training_loss, track_test=track_test_loss, save_path=root_path)\n",
    "        plot(epoch, track_training=track_training_accuracy, track_test=track_test_accuracy, save_path=root_path, loss=False)\n",
    "        save_model(model=model, epoch=epoch, accuracy=test_accuracy, loss=test_loss, save_path=root_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#version = 'b1'\n",
    "#width_mult, depth_mult, res, dropout_rate = efficient_net_config[version]\n",
    "#model = EfficientNet(width_mult, depth_mult, dropout_rate, num_classes = num_classes)\n",
    "#model = model.to(device)\n",
    "\n",
    "#epoch_num = 50\n",
    "#lr = 1e-3\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "#scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=epoch_num)\n",
    "\n",
    "#train(model = model,\n",
    "#      criterion = criterion,\n",
    "#        optimizer = optimizer,\n",
    "#        scheduler = scheduler, \n",
    "#        num_of_epochs = epoch_num,\n",
    "#        root_path='/run/media/tulitanir/Новый том/efficientnet_output/b1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "unload_model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
